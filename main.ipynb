{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Estimation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DEVELOPMENT = \"Dataset/development.csv\"\n",
    "FILE_EVALUATION = \"Dataset/evaluation.csv\"\n",
    "AUDIOS_DEVELOPMENT = \"Dataset/audios_development/\"\n",
    "AUDIOS_EVALUATION = \"Dataset/audios_evaluation/\"\n",
    "\n",
    "COMPUTE_ALL = False\n",
    "\n",
    "precisionLog = {\n",
    "    'mean_pitch':2, \n",
    "    'max_pitch':2, \n",
    "    'min_pitch':3, \n",
    "    'jitter':2, \n",
    "    'shimmer':2, \n",
    "    'energy':1, \n",
    "    'zcr_mean':2, \n",
    "    'spectral_centroid_mean':2,\n",
    "    'tempo':3,\n",
    "    'hnr':0\n",
    "}\n",
    "\n",
    "precisionLinear = {\n",
    "    'mean_pitch':-1, \n",
    "    'max_pitch':-2, \n",
    "    'min_pitch':0, \n",
    "    'jitter':3, \n",
    "    'shimmer':3, \n",
    "    'energy':3, \n",
    "    'zcr_mean':2, \n",
    "    'spectral_centroid_mean':-2,\n",
    "    'tempo':3,\n",
    "    'hnr':0\n",
    "}\n",
    "\n",
    "columns_correlator = {\n",
    "    k:v for k,v in zip(\n",
    "        [\n",
    "            \"time_domain.csv\", \"freq_domain.csv\", \"spectrogram.csv\", \"mel_spectrogram.csv\", \"mfcc.csv\", \"hpss.csv\", \"zcr.csv\", \"poly.csv\", \"fundamental_freq.csv\"\n",
    "        ], \n",
    "        [\n",
    "            ['audio_length'], \n",
    "            ['dominant_frequency', 'highest_freq_in_spectrum', 'lowest_freq_in_spectrum', 'spectrum_wide'], \n",
    "            ['spect_overall_mean', 'spect_overall_var'] + [f\"spect_frequency_mean_{t}\" for t in range(32)] + [f\"spect_frequency_var_{t}\" for t in range(32)],\n",
    "            ['mel_overall_mean', 'mel_overall_var'] + [f\"mel_frequency_mean_{t}\" for t in range(32)] + [f\"mel_frequency_var_{t}\" for t in range(32)],\n",
    "            ['mfcc_overall_mean'] + [f\"mfcc_frequency_mean_{t}\" for t in range(32)] + [f\"mfcc_frequency_var_{t}\" for t in range(32)],\n",
    "            ['harmonic_overall_mean', 'harmonic_overall_var', 'percussion_overall_mean', 'percussion_overall_var'] + [f\"harmonic_frequency_mean_{t}\" for t in range(32)] + [f\"harmonic_frequency_var_{t}\" for t in range(32)] + [f\"percussion_frequency_mean_{t}\" for t in range(32)] + [f\"percussion_frequency_var_{t}\" for t in range(32)],\n",
    "            ['zcr'],\n",
    "            ['mean_coeffs', 'std_coeffs'],\n",
    "            ['fundamental_freq']\n",
    "        ]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_original_df = pd.read_csv(FILE_DEVELOPMENT, header=0, index_col=0)\n",
    "eval_original_df = pd.read_csv(FILE_EVALUATION, header=0, index_col=0)\n",
    "\n",
    "audio_dev = os.listdir(AUDIOS_DEVELOPMENT)\n",
    "audio_eval = os.listdir(AUDIOS_EVALUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio_dev[:5])\n",
    "print(audio_eval[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order audios by integers and increasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dev.sort(key=lambda x: int(x.split('.')[0]))\n",
    "audio_eval.sort(key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "print(audio_dev[:5])\n",
    "print(audio_eval[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dev_original_df.head())\n",
    "\n",
    "display(f\"Total number of nan in development: {dev_original_df.isna().sum().sum()}\")\n",
    "display(f\"Total number of nan in evaluation: {eval_original_df.isna().sum().sum()}\")\n",
    "\n",
    "desc_dev_df = dev_original_df.describe()\n",
    "desc_eval_df = eval_original_df.describe()\n",
    "\n",
    "sampling_rate = dev_original_df['sampling_rate'].iloc[0]\n",
    "\n",
    "display(desc_dev_df.loc['std', 'sampling_rate'])\n",
    "display(desc_eval_df.loc['std', 'sampling_rate'])\n",
    "\n",
    "dev_df = dev_original_df.drop('sampling_rate', axis=1)\n",
    "eval_df = eval_original_df.drop('sampling_rate', axis=1)\n",
    "\n",
    "ages_df = dev_df[['age']]\n",
    "path_dev_df = dev_df[['path']]\n",
    "path_eval_df = eval_df[['path']]\n",
    "\n",
    "dev_df = dev_df.drop(['path'], axis=1)\n",
    "eval_df = eval_df.drop('path', axis=1)\n",
    "\n",
    "display(dev_df.head())\n",
    "display(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = dev_df['ethnicity'].value_counts()\n",
    "et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_english(X_df:pd.DataFrame):\n",
    "    X_copy = X_df.copy()\n",
    "\n",
    "    X_copy['english'] = (X_copy['ethnicity'] == 'english').astype(int)\n",
    "    X_copy['others'] = (X_copy['ethnicity'] != 'english').astype(int)\n",
    "\n",
    "    return X_copy.drop(columns='ethnicity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ethnicity(X_df, ethnie):\n",
    "    aggregated_ethnicity_df = X_df.copy()\n",
    "\n",
    "    # etna = pd.DataFrame(1, columns=ethnie, index=X_df.index, dtype=float)\n",
    "\n",
    "    # aggregated_ethnicity_df.loc[~(aggregated_ethnicity_df['ethnicity'].isin(ethnie)), 'ethnicity'] = 'others'\n",
    "\n",
    "    # for etnia in ethnie:\n",
    "    #     aggregated_ethnicity_df.loc[aggregated_ethnicity_df['ethnicity'] == etnia, etnia] = etna[etnia]\n",
    "\n",
    "    #     # aggregated_ethnicity_df.loc[aggregated_ethnicity_df['ethnicity'] == etnia, etnia] = 1\n",
    "    #     aggregated_ethnicity_df.loc[~(aggregated_ethnicity_df['ethnicity'] == etnia), etnia] = 0\n",
    "\n",
    "    # # encoded_ethnicity = pd.get_dummies(aggregated_ethnicity_df['ethnicity'], dtype=int)\n",
    "\n",
    "    try:\n",
    "        aggregated_ethnicity_df = aggregated_ethnicity_df.drop(columns=['ethnicity'], axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    return aggregated_ethnicity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enticity_df = dev_df['ethnicity'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "median_et = enticity_df.median()\n",
    "display(enticity_df[enticity_df == 1].shape)\n",
    "display(enticity_df.head(20))\n",
    "display(enticity_df.tail(10))\n",
    "# enticity_df[enticity_df > median_et].plot()\n",
    "\n",
    "etnie_chosen = sorted(list(set(enticity_df[enticity_df > median_et].index))) + ['others']\n",
    "display(etnie_chosen)\n",
    "display(etnie_chosen.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_gender_one_hot(X_df:pd.DataFrame):\n",
    "    X_copy = X_df.copy()\n",
    "\n",
    "    X_copy['male'] = (X_copy['gender'] == 'male').astype(int)\n",
    "    X_copy['female'] = (X_copy['gender'].isin(['female','famale'])).astype(int)\n",
    "\n",
    "    return X_copy.drop(columns='gender', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenderMapper = {\n",
    "    'male': 1,\n",
    "    'female': -1\n",
    "}\n",
    "\n",
    "def encode_gender(X_df:pd.DataFrame, mapper):\n",
    "    encoded_df = X_df.copy()\n",
    "    for gender, value in mapper.items():\n",
    "        encoded_df.loc[encoded_df['gender'] == gender, 'gender'] = value\n",
    "\n",
    "    encoded_df.loc[encoded_df['gender'] == 'famale', 'gender'] = GenderMapper['female']\n",
    "\n",
    "    encoded_df['gender'] = encoded_df['gender'].astype(float)\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(encoded_gender_etnicity_df['tempo'])\n",
    "\n",
    "def encode_tempo(X_df):\n",
    "    encoded_df = X_df.copy()\n",
    "    encoded_df['tempo'] = encoded_df['tempo'].map(lambda x: float(x.strip('[').strip(']')))\n",
    "    return encoded_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(eval_df.loc[eval_df['ethnicity'].isin(etnie_chosen), 'ethnicity'].value_counts())\n",
    "\n",
    "# FOR NOW, WE WILL DROP ETHNICITY #TODO:\n",
    "\n",
    "# step1_dev_df = dev_df.drop(columns='ethnicity', axis=1)\n",
    "# step1_eval_df = eval_df.drop(columns='ethnicity', axis=1)\n",
    "\n",
    "# step1_dev_df = encode_english(dev_df)\n",
    "# step1_eval_df = encode_english(eval_df)\n",
    "\n",
    "step1_dev_df = encode_ethnicity(dev_df, etnie_chosen)\n",
    "step1_eval_df = encode_ethnicity(eval_df, etnie_chosen)\n",
    "\n",
    "# step2_dev_df = encode_gender_one_hot(step1_dev_df)\n",
    "# step2_eval_df = encode_gender_one_hot(step1_eval_df)\n",
    "\n",
    "step2_dev_df = encode_gender(step1_dev_df, GenderMapper)\n",
    "step2_eval_df = encode_gender(step1_eval_df, GenderMapper)\n",
    "\n",
    "step3_dev_df = encode_tempo(step2_dev_df)\n",
    "step3_eval_df = encode_tempo(step2_eval_df)\n",
    "\n",
    "display(step3_dev_df.head(5))\n",
    "display(step3_eval_df.head(5))\n",
    "\n",
    "# display(step3_eval_df[step3_eval_df['yoruba'] == 1])\n",
    "# display(eval_df.loc[75, :])\n",
    "\n",
    "display(step3_dev_df.describe())\n",
    "display(step3_eval_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavior analysis and $log_{10}$ scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colors(values:pd.Series, label:str, use_continous:bool=False, \n",
    "                  cmap_continuous = cm.viridis, cmap_discrete = cm.rainbow):\n",
    "    if use_continous:\n",
    "        cmap_ = cmap_continuous\n",
    "        # Alternative normalization without iterquartile ranges:\n",
    "        norm = plt.Normalize(vmin=np.min(values), vmax=np.max(values)) \n",
    "        \n",
    "        mappable = cm.ScalarMappable(norm=norm, cmap=cmap_)\n",
    "        handles = None\n",
    "    else:\n",
    "        cmap_ = cmap_discrete\n",
    "        norm = plt.Normalize(vmin=np.min(values), vmax=np.max(values)) \n",
    "        \n",
    "        mappable = None\n",
    "        handles = [\n",
    "            Line2D(\n",
    "                [0], [0], marker='o', color='none', linestyle='None', \n",
    "                markeredgewidth=0, markerfacecolor=cmap_(norm(val)), \n",
    "                markersize=10, label=label\n",
    "            ) for label, val in GenderMapper.items()\n",
    "        ]\n",
    "\n",
    "    return cmap_, norm, mappable, handles \n",
    "\n",
    "def round_column(data: pd.DataFrame, col: str, isLog:bool = True):\n",
    "    if col != 'hnr':\n",
    "        if isLog:\n",
    "            rounded_col = np.round(\n",
    "                np.log10(data[col]), \n",
    "                precisionLog[col]\n",
    "            )\n",
    "        else:\n",
    "            rounded_col = np.round(\n",
    "                data[col],\n",
    "                precisionLinear[col]\n",
    "            )\n",
    "    else:\n",
    "        rounded_col = np.round(data[col], precisionLinear[col])\n",
    "    return rounded_col\n",
    "\n",
    "\n",
    "def perform_aggregation(partition_df, target, gender, descrete):\n",
    "    grouped_temp = partition_df.groupby(col).agg({\n",
    "        'frequency': 'first',\n",
    "    })\n",
    "\n",
    "    if not descrete:\n",
    "        grouped_temp['target'] = grouped_temp.index.map(\n",
    "            lambda val: target.loc[partition_df[partition_df[col] == val].index, :].mean().iloc[0]\n",
    "        )\n",
    "    else:\n",
    "        grouped_temp['target'] = GenderMapper[gender]\n",
    "\n",
    "    grouped_temp = grouped_temp.reset_index()\n",
    "\n",
    "    return grouped_temp\n",
    "    \n",
    "def plot_distribution(fig:plt.Figure, ax:plt.Axes, X_df:pd.DataFrame, col:str, target:pd.Series, isLog:bool=True, descrete:bool=False):    \n",
    "    temp = X_df[['gender', col]].copy()\n",
    "\n",
    "    temp[col] = round_column(temp, col, isLog=isLog)\n",
    "\n",
    "    male_df = temp[temp['gender'] == GenderMapper['male']].copy()\n",
    "    female_df = temp[temp['gender'] == GenderMapper['female']].copy()\n",
    "\n",
    "    male_df['frequency'] = male_df[col].map(male_df[col].value_counts())\n",
    "    female_df['frequency'] = female_df[col].map(female_df[col].value_counts())\n",
    "\n",
    "    male_grouped_df = perform_aggregation(male_df, target, 'male', descrete)\n",
    "    female_grouped_df = perform_aggregation(female_df, target, 'female', descrete)\n",
    "\n",
    "    female_grouped_df.index = female_grouped_df.index + male_grouped_df.index[-1]\n",
    "\n",
    "    combined_df = pd.concat([male_grouped_df, female_grouped_df]).reset_index()\n",
    "\n",
    "    cmap_, norm, mappable, handles = create_colors(combined_df.loc[:, 'target'], label='gender', use_continous=not descrete)\n",
    "\n",
    "    sc = ax.scatter(combined_df[col], \n",
    "                    combined_df['frequency'], \n",
    "                    c=cmap_(norm(combined_df['target'])), \n",
    "                    alpha=0.7)\n",
    "\n",
    "    if mappable:\n",
    "        fig.colorbar(mappable=mappable, ax=ax)\n",
    "    else:\n",
    "        ax.legend(handles=handles)\n",
    "\n",
    "#TODO: without 'gender' column does not work\n",
    "def tabular_data_distribution():\n",
    "    for i in range(9):\n",
    "        col = list(precisionLog.keys())[i]\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "        fig.suptitle(f'Distribution of {col.capitalize()} in log10 scale')\n",
    "\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        plot_distribution(fig, ax1, step3_dev_df, col, ages_df, isLog=True, descrete=True)\n",
    "        ax1.set_xlabel(f'log_10({col.capitalize()})')\n",
    "        ax1.set_ylabel(f'Count')\n",
    "        \n",
    "        ax2 = fig.add_subplot(122)\n",
    "        ax2.set_title(\"Points colored with average age of grouped points\")\n",
    "\n",
    "        plot_distribution(fig, ax2, step3_dev_df, col, ages_df, isLog=True, descrete=False)\n",
    "        ax2.set_xlabel(f'log_10({col.capitalize()})')\n",
    "        ax1.set_ylabel(f'Count')\n",
    "        plt.show()\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "        fig.suptitle(f'Distribution of {col.capitalize()} in linear scale')\n",
    "\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        plot_distribution(fig, ax1, step3_dev_df, col, None, isLog=False, descrete=True)\n",
    "        ax1.set_xlabel(f'{col.capitalize()}')\n",
    "        ax2.set_ylabel(f'Count')\n",
    "        \n",
    "        ax2 = fig.add_subplot(122)\n",
    "        ax2.set_title(\"Points colored with average age of grouped points\")\n",
    "        plot_distribution(fig, ax2, step3_dev_df, col, ages_df, isLog=False, descrete=False)\n",
    "        ax2.set_ylabel(f'Count')\n",
    "        ax2.set_xlabel(f'{col.capitalize()}')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    col = list(precisionLog.keys())[9]\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    fig.suptitle(f'Distribution of {col.capitalize()} in linear scale')\n",
    "\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    plot_distribution(fig, ax1, step3_dev_df, col, None, isLog=False, descrete=True)\n",
    "    ax1.set_xlabel(f'{col.capitalize()}')\n",
    "    ax1.set_ylabel(f'Count')\n",
    "\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.set_title(\"Points colored with average age of grouped points\")\n",
    "    plot_distribution(fig, ax2, step3_dev_df, col, ages_df, isLog=False, descrete=False)\n",
    "    ax2.set_xlabel(f'{col.capitalize()}')\n",
    "    ax2.set_ylabel(f'Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3_log_dev_df = step3_dev_df.copy()\n",
    "step3_log_eval_df = step3_eval_df.copy()\n",
    "\n",
    "for i, toSub in zip(precisionLog.keys(), [1, 0, 0, 1, 1, 1, 1, 1, 0, 0]):\n",
    "    if toSub == 1:\n",
    "        step3_log_dev_df.loc[:, i] = np.log10(step3_dev_df.loc[:, i])\n",
    "        step3_log_eval_df.loc[:, i] = np.log10(step3_eval_df.loc[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp:pd.Series = step3_log_dev_df.corr().loc['age', :].sort_values(ascending=False)\n",
    "display(temp.head(10))\n",
    "display(temp.tail(10))\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot()\n",
    "sns.heatmap(np.abs(step3_log_dev_df.corr()), ax=ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_age_dev_df = step3_log_dev_df.drop(columns=['age'], axis=1)\n",
    "\n",
    "scaler = StandardScaler().fit(no_age_dev_df)\n",
    "\n",
    "step3_log_norm_dev_df = pd.DataFrame(scaler.transform(no_age_dev_df), columns=no_age_dev_df.columns)\n",
    "step3_log_norm_eval_df = pd.DataFrame(scaler.transform(step3_log_eval_df), columns=step3_log_eval_df.columns)\n",
    "\n",
    "display(step3_log_norm_dev_df.head())\n",
    "display(step3_log_norm_eval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_distripution(y_pred, y_val, precision):\n",
    "    errors = pd.DataFrame(np.round(y_pred.flatten() - y_val.values.flatten(), precision), columns=['error'])\n",
    "    error_counts = errors['error'].value_counts().reset_index()\n",
    "    error_counts.columns = ['error', 'count']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(error_counts['error'], error_counts['count'])\n",
    "    plt.xlabel('Error')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Prediction Errors')\n",
    "    plt.show()\n",
    "    return error_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross_val_score(make_pipeline(StandardScaler(), RandomForestRegressor()), step3_log_dev_df, ages_df, cv=15, scoring='neq_mean_squared_error').mean().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_df = step3_log_norm_dev_df.copy()\n",
    "# X_train_val_df = X_train_val_df.drop(columns=[], axis=1)\n",
    "\n",
    "forest = RandomForestRegressor(random_state=341967)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val_df, ages_df, test_size=0.3, shuffle=True, random_state=341967)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_val)\n",
    "\n",
    "display(root_mean_squared_error(y_val, y_pred))\n",
    "\n",
    "_ = plot_error_distripution(y_pred, y_val, 0)\n",
    "#10.455056281368881"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look to feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(forest.feature_names_in_, forest.feature_importances_), key=lambda x:x[1] , reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only ethnicity \"english\" seems to give a significant contribution to regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3_log_norm_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval_pred = forest.predict(X_train_val_df.values)\n",
    "\n",
    "display(np.max(y_eval_pred), np.min(y_eval_pred))\n",
    "display(np.max(ages_df), np.min(ages_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(\n",
    "    cross_val_score(\n",
    "        RandomForestRegressor(n_estimators=400, n_jobs=-1, random_state=341967), \n",
    "        X_train_val_df, \n",
    "        ages_df, \n",
    "        cv=5, \n",
    "        scoring='neg_root_mean_squared_error', \n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    ")\n",
    "\n",
    "# abs(\n",
    "#     croscross_val_score(\n",
    "#         make_pipeline(StandardScaler(), RandomForestRegressor()), \n",
    "#         X_train_val_df, \n",
    "#         ages_df, cv=10, \n",
    "#         scoring='neg_root_mean_squared_error', \n",
    "#         n_jobs=-1\n",
    "#     ).mean()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grid(builder, configs, cv):\n",
    "#     gs = GridSearchCV(builder(), configs, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv)\n",
    "#     gs.fit(X_train.values, y_train.values.reshape((-1,)))\n",
    "    \n",
    "#     return gs\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val_df, ages_df, test_size=0.2, shuffle=True, random_state=341967)\n",
    "\n",
    "# best = grid(RandomForestRegressor, {'n_estimators': [300, 500], 'random_state': [341967]}, 5)\n",
    "\n",
    "# y_pred = best.predict(X_val.values)\n",
    "\n",
    "# display(root_mean_squared_error(y_val, y_pred))\n",
    "# _ = plot_error_distripution(y_pred, y_val, 0)\n",
    "\n",
    "#10.424189101247185"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio(y:np.ndarray, sr:float, time=False, freq=False, spectrogram=False, mel=False, mfcc=False, hp=False, poly=False,plot=True):\n",
    "    \"\"\"\n",
    "        Returns the thing plotted\\n\n",
    "        If freq = True, returns the frequencies used too\\n\n",
    "        If hp=True, returns Harmonic and Percussion\\n\n",
    "        If all False, returns y\n",
    "    \"\"\"\n",
    "    target_freq_bins = 32\n",
    "    n_fft = (target_freq_bins - 1) * 2\n",
    "\n",
    "    if time:\n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 3))\n",
    "            plt.title(f\"Audio {i} as waveform\")\n",
    "            librosa.display.waveshow(y, sr=sr)\n",
    "\n",
    "        return y\n",
    "    if freq:\n",
    "        y_freq = np.abs(scipy.fft.fft(y))\n",
    "        f = np.linspace(0, sr, len(y_freq))\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 3))\n",
    "            plt.title(f\"Spectrum of audio {i}\")\n",
    "            plt.semilogx(f[: len(f) // 2], y_freq[: len(f) // 2])\n",
    "            plt.xlabel(\"Frequency (Hz)\")\n",
    "\n",
    "        return y_freq, f\n",
    "    if spectrogram:    \n",
    "        y_stft = np.abs(librosa.stft(y, n_fft=n_fft))\n",
    "        y_stft_db = librosa.amplitude_to_db(y_stft, ref=np.max)\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.title(f\"Spectrogram of audio {i}\")\n",
    "            librosa.display.specshow(y_stft_db, sr=sr, x_axis=\"time\", y_axis=\"log\")\n",
    "            plt.colorbar(format=\"%.1f dB\")\n",
    "\n",
    "        return y_stft, y_stft_db\n",
    "    if mel:\n",
    "        y_mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=32)\n",
    "        y_mel_db = librosa.power_to_db(y_mel, ref=np.max)\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.title(f\"Mel-spectrogram of audio {i}\")\n",
    "            librosa.display.specshow(y_mel_db, sr=sr, x_axis=\"time\", y_axis=\"mel\")\n",
    "            plt.colorbar(format=\"%.2f dB\")\n",
    "        \n",
    "        return y_mel, y_mel_db\n",
    "    if mfcc:\n",
    "        y_mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=32)\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.title(f\"MFCC of audio {i}\")\n",
    "            librosa.display.specshow(y_mfccs, sr=sr, x_axis=\"time\")\n",
    "            plt.colorbar()\n",
    "\n",
    "        return y_mfccs\n",
    "    if hp:\n",
    "        y_stft = librosa.stft(y, n_fft=n_fft)\n",
    "        H, P = librosa.decompose.hpss(y_stft)\n",
    "        Hmag = np.abs(H)\n",
    "        Pmag = np.abs(P)\n",
    "\n",
    "        if plot:\n",
    "            Hdb = librosa.amplitude_to_db(Hmag, ref=np.max)\n",
    "            Pdb = librosa.amplitude_to_db(Pmag, ref=np.max)\n",
    "            \n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.title(f\"Harmonic source of audio {i}\")\n",
    "            librosa.display.specshow(Hdb, sr=sr, x_axis=\"time\", y_axis=\"log\")\n",
    "            plt.colorbar(format=\"%.2f dB\")\n",
    "\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.title(f\"Percussive source of audio {i}\")\n",
    "            librosa.display.specshow(Pdb, sr=sr, x_axis=\"time\", y_axis=\"log\")\n",
    "            plt.colorbar(format=\"%.2f dB\")\n",
    "\n",
    "        return librosa.istft(H), Hmag, librosa.istft(P), Pmag\n",
    "    if poly:\n",
    "        y_poly = librosa.feature.poly_features(y=y, sr=sr)\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.title(f\"Polynomial feature of audio {i}\")\n",
    "            plt.plot(y_poly[0], label=\"Coeffs order 0\")\n",
    "            plt.plot(y_poly[1], label=\"Coeffs order 1\")\n",
    "            plt.legend()\n",
    "\n",
    "        return y_poly\n",
    "    plt.show()\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_domain(dir, audio, plot=False):\n",
    "    res = pd.DataFrame(0, index=[audio], columns=['audio_length'], dtype=float)\n",
    "\n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    length = y.shape[0]/sr\n",
    "\n",
    "    res.loc[audio, 'audio_length'] = length\n",
    "\n",
    "    if plot:\n",
    "        plot_audio(y, sr, time=True, plot=plot)\n",
    "\n",
    "        print(f\"Audio: {i}\")\n",
    "        print(f\"Length: {length}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_domain(dir, audio, plot=False):\n",
    "    res = pd.DataFrame(0, index=[audio], columns=['dominant_frequency', 'highest_freq_in_spectrum', 'lowest_freq_in_spectrum', 'spectrum_wide'], dtype=float)\n",
    "\n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    y_freq, f = plot_audio(y, sr, freq=True, plot=plot)\n",
    "\n",
    "    y_mag = np.abs(y_freq)\n",
    "    dominant_frequency = f[np.argmax(y_mag)]\n",
    "\n",
    "    threshold = np.max(y_mag) * 0.1\n",
    "    significant_freqs = f[y_mag > threshold]\n",
    "\n",
    "    if len(significant_freqs) > 0:\n",
    "        lowest_freq_in_spectrum, highest_freq_in_spectrum = significant_freqs[0], significant_freqs[-1]\n",
    "    else:\n",
    "        lowest_freq_in_spectrum, highest_freq_in_spectrum = 0, 0\n",
    "\n",
    "    spectrum_wide = highest_freq_in_spectrum - lowest_freq_in_spectrum\n",
    "    res.loc[audio, 'dominant_frequency'] = dominant_frequency\n",
    "    res.loc[audio, 'lowest_freq_in_spectrum'] = lowest_freq_in_spectrum\n",
    "    res.loc[audio, 'highest_freq_in_spectrum'] = highest_freq_in_spectrum\n",
    "    res.loc[audio, 'spectrum_wide'] = spectrum_wide\n",
    "\n",
    "    if plot:\n",
    "        print(f\"Audio: {audio}\")\n",
    "        print(f\"Dominant frequency: {dominant_frequency}\")\n",
    "\n",
    "        print(f\"Lowest important freq:{lowest_freq_in_spectrum}\")\n",
    "        print(f\"Highest important freq:{highest_freq_in_spectrum}\")\n",
    "        print(f\"Length of spectrum: {highest_freq_in_spectrum - lowest_freq_in_spectrum}\\n\")\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(dir, audio, plot=False):\n",
    "    spect_frequency_mean_cols = [f\"spect_frequency_mean_{t}\" for t in range(32)]\n",
    "    spect_frequency_var_cols  = [f\"spect_frequency_var_{t}\" for t in range(32)]\n",
    "    spect_frequency_skew_cols = [f\"spect_frequency_skew_{t}\" for t in range(32)]\n",
    "    spect_frequency_kurt_cols = [f\"spect_frequency_kurt_{t}\" for t in range(32)]\n",
    "    \n",
    "    res = pd.DataFrame(0, index=[audio], columns=['spect_overall_mean', 'spect_overall_var'] + spect_frequency_mean_cols + spect_frequency_var_cols, dtype=float)\n",
    "\n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    S, Sdb= plot_audio(y, sr, spectrogram=True, plot=plot)\n",
    "\n",
    "    spect_temporal_mean = S.mean(axis=0)\n",
    "    spect_temporal_variance = S.var(axis=0)\n",
    "    spect_temporal_skewness = skew(S, axis=0)\n",
    "    spect_temporal_kurtosis = kurtosis(S, axis=0)\n",
    "\n",
    "    spect_frequency_mean = S.mean(axis=1)\n",
    "    spect_frequency_variance = S.var(axis=1)\n",
    "    spect_frequency_skewness = skew(S, axis=1)\n",
    "    spect_frequency_kurtosis = kurtosis(S, axis=1)\n",
    "\n",
    "    spect_overall_mean = S.mean()\n",
    "    spect_overall_var = S.var()\n",
    "\n",
    "    res.loc[audio, 'spect_overall_mean'] = spect_overall_mean\n",
    "    res.loc[audio, 'spect_overall_var'] = spect_overall_var\n",
    "\n",
    "    res.loc[audio, spect_frequency_mean_cols] = spect_frequency_mean\n",
    "    res.loc[audio, spect_frequency_var_cols] = spect_frequency_variance\n",
    "\n",
    "    if plot:\n",
    "        print(f\"Audio: {audio}\")\n",
    "        print(f\"Temporal mean:\\n{spect_temporal_mean}\")\n",
    "        print(f\"Temporal variance:\\n{spect_temporal_variance}\")\n",
    "        print(f\"Temporal skewness:\\n{spect_temporal_skewness}\")\n",
    "        print(f\"Temporal kurtosis:\\n{spect_temporal_kurtosis}\")\n",
    "        \n",
    "        print(f\"Overall mean:\\n{S.mean()}\")\n",
    "        print(f\"Overall variance:\\n{S.var()}\")\n",
    "        \n",
    "        print(f\"Frequency mean:\\n{spect_frequency_mean}\")\n",
    "        print(f\"Frequency variance:\\n{spect_frequency_variance}\")\n",
    "        print(f\"Frequency skewness:\\n{spect_frequency_skewness}\")\n",
    "        print(f\"Frequency kurtosis:\\n{spect_frequency_kurtosis}\")\n",
    "\n",
    "        display(spect_temporal_mean.shape)    \n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel-spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_spectrogram(dir, audio, plot=False):\n",
    "    mel_frequency_mean_cols = [f\"mel_frequency_mean_{t}\" for t in range(32)]\n",
    "    mel_frequency_var_cols  = [f\"mel_frequency_var_{t}\" for t in range(32)]\n",
    "    mel_frequency_skew_cols = [f\"mel_frequency_skew_{t}\" for t in range(32)]\n",
    "    mel_frequency_kurt_cols = [f\"mel_frequency_kurt_{t}\" for t in range(32)]\n",
    "\n",
    "    res = pd.DataFrame(0, index=[audio], columns=['mel_overall_mean', 'mel_overall_var']+mel_frequency_mean_cols+mel_frequency_var_cols, dtype=float)\n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    S, S_db = plot_audio(y, sr, mel=True, plot=plot)\n",
    "\n",
    "    mel_temporal_mean =     S.mean(axis=0)\n",
    "    mel_temporal_variance = S.var(axis=0)\n",
    "    mel_temporal_skewness = skew(S, axis=0)\n",
    "    mel_temporal_kurtosis = kurtosis(S, axis=0)\n",
    "\n",
    "    # mel_temporal_mean.columns = mel_temporal_mean_cols\n",
    "    # mel_temporal_variance.columns = [f\"mel_temporal_var_{t}\" for t in range(S.shape[0])]\n",
    "    # mel_temporal_skewness.columns = [f\"mel_temporal_skew_{t}\" for t in range(S.shape[0])]\n",
    "    # mel_temporal_kurtosis.columns = [f\"mel_temporal_kurt_{t}\" for t in range(S.shape[0])]\n",
    "\n",
    "    mel_frequency_mean = S.mean(axis=1)\n",
    "    mel_frequency_variance = S.var(axis=1)\n",
    "    mel_frequency_skewness = skew(S, axis=1)\n",
    "    mel_frequency_kurtosis = kurtosis(S, axis=1)\n",
    "\n",
    "    overall_mean = S.mean()\n",
    "    overall_var = S.var()\n",
    "\n",
    "    res.loc[audio, 'mel_overall_mean'] = overall_mean\n",
    "    res.loc[audio, 'mel_overall_var'] = overall_var\n",
    "\n",
    "    res.loc[audio, mel_frequency_mean_cols] = mel_frequency_mean\n",
    "    res.loc[audio, mel_frequency_var_cols] = mel_frequency_variance\n",
    "\n",
    "    # if plot:\n",
    "    #     print(f\"Audio: {audio}\")\n",
    "        \n",
    "    #     print(f\"Overall mean:\\n{overall_mean}\")\n",
    "    #     print(f\"Overall variance:\\n{overall_var}\")\n",
    "\n",
    "    #     print(f\"Temporal mean:    \\n{mel_temporal_mean}\")\n",
    "    #     print(f\"Temporal variance:\\n{mel_temporal_variance}\")\n",
    "    #     print(f\"Temporal skewness:\\n{mel_temporal_skewness}\")\n",
    "    #     print(f\"Temporal kurtosis:\\n{mel_temporal_kurtosis}\")\n",
    "\n",
    "    #     print(f\"Frequency mean:    \\n{mel_frequency_mean}\")\n",
    "    #     print(f\"Frequency variance:\\n{mel_frequency_variance}\")\n",
    "    #     print(f\"Frequency skewness:\\n{mel_frequency_skewness}\")\n",
    "    #     print(f\"Frequency kurtosis:\\n{mel_frequency_kurtosis}\")\n",
    "    #     display('')\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel-frequency cepstral coefficients (MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc(dir, audio, plot=False):\n",
    "    n_mfcc = 32\n",
    "    mfcc_frequency_mean_cols = [f\"mfcc_frequency_mean_{t}\" for t in range(n_mfcc)]\n",
    "    mfcc_frequency_var_cols  = [f\"mfcc_frequency_var_{t}\"  for t in range(n_mfcc)]\n",
    "    mfcc_frequency_skew_cols = [f\"mfcc_frequency_skew_{t}\" for t in range(n_mfcc)]\n",
    "    mfcc_frequency_kurt_cols = [f\"mfcc_frequency_kurt_{t}\" for t in range(n_mfcc)]\n",
    "\n",
    "    res = pd.DataFrame(0, index=[audio], columns=['mfcc_overall_mean', 'mfcc_overall_var']+mfcc_frequency_mean_cols+mfcc_frequency_var_cols, dtype=float)\n",
    "\n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    S = plot_audio(y, sr, mfcc=True, plot=plot)\n",
    "\n",
    "    mfcc_temporal_mean     = S.mean(axis=0)\n",
    "    mfcc_temporal_variance = S.var(axis=0)\n",
    "    mfcc_temporal_skewness = skew(S, axis=0)\n",
    "    mfcc_temporal_kurtosis = kurtosis(S, axis=0)\n",
    "\n",
    "    mfcc_frequency_mean = S.mean(axis=1)\n",
    "    mfcc_frequency_variance = S.var(axis=1)\n",
    "    mfcc_frequency_skewness = skew(S, axis=1)\n",
    "    mfcc_frequency_kurtosis = kurtosis(S, axis=1)\n",
    "\n",
    "    # print(temporal_mean)\n",
    "\n",
    "    overall_mean = S.mean()\n",
    "    overall_var = S.var()\n",
    "\n",
    "    res.loc[audio, 'mfcc_overall_mean'] = overall_mean\n",
    "    # res.loc[audio, 'mfcc_overall_var'] = overall_var\n",
    "\n",
    "    res.loc[audio, mfcc_frequency_mean_cols] = mfcc_frequency_mean\n",
    "    res.loc[audio, mfcc_frequency_var_cols] = mfcc_frequency_variance\n",
    "    \n",
    "    if plot:\n",
    "        print(f\"Audio: {audio}\")\n",
    "        print(f\"Temporal mean:\\n{temporal_mean}\")\n",
    "        print(f\"Temporal variance:\\n{temporal_variance}\")\n",
    "        print(f\"Temporal skewness:\\n{temporal_skewness}\")\n",
    "        print(f\"Temporal kurtosis:\\n{temporal_kurtosis}\")\n",
    "        \n",
    "        print(f\"Overall mean:\\n{overall_mean}\")\n",
    "        print(f\"Overall variance:\\n{overall_var}\")\n",
    "        \n",
    "        print(f\"Frequency mean:\\n{frequency_mean}\")\n",
    "        print(f\"Frequency variance:\\n{frequency_variance}\")\n",
    "        print(f\"Frequency skewness:\\n{frequency_skewness}\")\n",
    "        print(f\"Frequency kurtosis:\\n{frequency_kurtosis}\")\n",
    "    \n",
    "        display(\"\")\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmonic-percussive source separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpss(dir, audio, plot=False):\n",
    "    harmonic_frequency_mean_cols = [f\"harmonic_frequency_mean_{t}\" for t in range(32)]\n",
    "    harmonic_frequency_var_cols = [f\"harmonic_frequency_var_{t}\" for t in range(32)]\n",
    "    harmonic_frequency_skew_cols = [f\"harmonic_frequency_skew_{t}\" for t in range(32)]\n",
    "    harmonic_frequency_kurt_cols = [f\"harmonic_frequency_kurt_{t}\" for t in range(32)]\n",
    "\n",
    "    percussion_frequency_mean_cols = [f\"percussion_frequency_mean_{t}\" for t in range(32)]\n",
    "    percussion_frequency_var_cols = [f\"percussion_frequency_var_{t}\" for t in range(32)]\n",
    "    percussion_frequency_skew_cols = [f\"percussion_frequency_skew_{t}\" for t in range(32)]\n",
    "    percussion_frequency_kurt_cols = [f\"percussion_frequency_kurt_{t}\" for t in range(32)]\n",
    "    \n",
    "    res = pd.DataFrame(\n",
    "        0, \n",
    "        index=[audio], \n",
    "        columns=\n",
    "            ['harmonic_overall_mean', 'harmonic_overall_var', 'percussion_overall_mean', 'percussion_overall_var']+\n",
    "            harmonic_frequency_mean_cols+\n",
    "            harmonic_frequency_var_cols+\n",
    "            percussion_frequency_mean_cols+\n",
    "            percussion_frequency_var_cols,\n",
    "        dtype=float\n",
    "    )\n",
    "\n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    h, Hmag, p, Pmag = plot_audio(y, sr, hp=True, plot=plot)\n",
    "    \n",
    "    harmonic_frequency_mean     = Hmag.mean(axis=1)\n",
    "    harmonic_frequency_variance = Hmag.var(axis=1)\n",
    "    harmonic_frequency_skewness = skew(Hmag, axis=1)\n",
    "    harmonic_frequency_kurtosis = kurtosis(Hmag, axis=1)\n",
    "\n",
    "    percussion_frequency_mean =     Pmag.mean(axis=1)\n",
    "    percussion_frequency_variance = Pmag.var(axis=1)\n",
    "    percussion_frequency_skewness = skew(Pmag, axis=1)\n",
    "    percussion_frequency_kurtosis = kurtosis(Pmag, axis=1)\n",
    "\n",
    "    harmonic_overall_mean = Hmag.mean()\n",
    "    harmonic_overall_var = Hmag.var()\n",
    "\n",
    "    percussion_overall_mean = Pmag.mean()\n",
    "    percussion_overall_var = Pmag.var()\n",
    "\n",
    "    res.loc[audio, 'harmonic_overall_mean'] = harmonic_overall_mean\n",
    "    res.loc[audio, 'harmonic_overall_var'] = harmonic_overall_var        \n",
    "    res.loc[audio, 'percussion_overall_mean'] = percussion_overall_mean\n",
    "    res.loc[audio, 'percussion_overall_var'] = percussion_overall_var\n",
    "\n",
    "    res.loc[audio, harmonic_frequency_mean_cols] = harmonic_frequency_mean\n",
    "    res.loc[audio, harmonic_frequency_var_cols] = harmonic_frequency_variance\n",
    "    res.loc[audio, percussion_frequency_mean_cols] = percussion_frequency_mean\n",
    "    res.loc[audio, percussion_frequency_var_cols] = percussion_frequency_variance\n",
    "\n",
    "    if plot:\n",
    "        print(f\"Temporal mean for harmonic source:    \\n{temporal_mean_h}\")\n",
    "        print(f\"Temporal variance for harmonic source:\\n{temporal_variance_h}\")\n",
    "        print(f\"Temporal skewness for harmonic source:\\n{temporal_skewness_h}\")\n",
    "        print(f\"Temporal kurtosis for harmonic source:\\n{temporal_kurtosis_h}\")\n",
    "        \n",
    "        print(f\"Temporal mean for percussion source:    \\n{temporal_mean_p}\")\n",
    "        print(f\"Temporal variance for percussion source:\\n{temporal_variance_p}\")\n",
    "        print(f\"Temporal skewness for percussion source:\\n{temporal_skewness_p}\")\n",
    "        print(f\"Temporal kurtosis for percussion source:\\n{temporal_kurtosis_p}\")\n",
    "\n",
    "        print(f\"Overall mean for harmonic source:\\n{harmonic_overall_mean}\")\n",
    "        print(f\"Overall variance for harmonic source:\\n{harmonic_overall_var}\")\n",
    "        print(f\"Overall mean for percussion source:\\n{percussion_overall_mean}\")\n",
    "        print(f\"Overall variance for percussion source:\\n{percussion_overall_var}\")\n",
    "\n",
    "        display(\"\")\n",
    "        \n",
    "        plot_audio(h, sr, mfcc=True, plot=plot)\n",
    "        plot_audio(p, sr, mfcc=True, plot=plot)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-crossing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr(dir, audio, plot=False):\n",
    "    res = pd.DataFrame(0, index=[audio], columns=['zcr'], dtype=float)\n",
    "    \n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    zero_astfly = librosa.zero_crossings(y, pad=False).sum()\n",
    "    zcr_ = float(zero_astfly/(y.shape[0]/sr))\n",
    "\n",
    "    res.loc[audio, 'zcr'] = zcr_\n",
    "\n",
    "    if plot:\n",
    "        display(zcr_)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polyfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(dir, audio, plot=False):\n",
    "    res = pd.DataFrame(0, index=[audio], columns=['mean_coeffs', 'std_coeffs'], dtype=float)\n",
    "    \n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    y_poly = plot_audio(y, sr, poly=True, plot=plot)\n",
    "\n",
    "    res.loc[audio, 'mean_coeffs'] = y_poly[1].mean()\n",
    "    res.loc[audio, 'std_coeffs'] = y_poly[1].std()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental_freq(dir, audio, plot=False):\n",
    "    res = pd.DataFrame(0, index=[audio], columns=['f0'], dtype=float)\n",
    "\n",
    "    y, sr = librosa.load(dir + audio, sr=sampling_rate)\n",
    "\n",
    "    fmin = librosa.note_to_hz('C2')\n",
    "    fmax = librosa.note_to_hz('C7')\n",
    "\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(y,fmin=fmin, fmax=fmax,n_thresholds=10 ,sr=sr)\n",
    "\n",
    "    f0 = f0[voiced_flag]\n",
    "    # print(f0.mean())\n",
    "\n",
    "    A = librosa.lpc(y=y, order=2+sr//1000)\n",
    "    rts = np.roots(A)\n",
    "    reals = np.imag(rts) >=0\n",
    "\n",
    "    if np.sum(reals) >=2:\n",
    "        rts = rts[reals]\n",
    "        angz = np.arctan2(np.imag(rts), np.real(rts))\n",
    "        frqs = angz * sr / (2 * np.pi)\n",
    "        frqs.sort()\n",
    "\n",
    "        f1 = frqs[0]\n",
    "        f2 = frqs[1]\n",
    "    else:\n",
    "        f1 = 0\n",
    "        f2 = 0\n",
    "\n",
    "    res.loc[audio, 'f0'] = f0.mean()\n",
    "    res.loc[audio, 'f1'] = f1\n",
    "    res.loc[audio, 'f2'] = f2\n",
    "\n",
    "    return res\n",
    "\n",
    "# for i in range(len(first_five)):\n",
    "#     fundamental_freq(AUDIOS_DEVELOPMENT, first_five[i], plot=False)\n",
    "#     print(f'Audio {i}')\n",
    "#     print('Gender: ', dev_df.iloc[i]['gender'])\n",
    "#     print('Age: ', ages_df.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tested audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audios = np.random.choice(audio_dev, 3)\n",
    "first_five = audio_dev[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.DataFrame()\n",
    "\n",
    "for audio in tqdm(test_audios):\n",
    "    # time = time_domain(AUDIOS_DEVELOPMENT, audio, plot=False)\n",
    "    # freq = freq_domain(AUDIOS_DEVELOPMENT, audio, plot=False)\n",
    "    # spec = spectrogram(AUDIOS_DEVELOPMENT, audio, plot=False)\n",
    "    # mel = mel_spectrogram(AUDIOS_DEVELOPMENT, audio, plot=False)\n",
    "    # mfccs = mfcc(AUDIOS_DEVELOPMENT, audio, plot=False)\n",
    "    # hp = hpss(AUDIOS_DEVELOPMENT, audio, plot=False)\n",
    "    # zcrs = zcr(AUDIOS_DEVELOPMENT, audio, plot=False)\n",
    "    # poly_feats = poly(AUDIOS_DEVELOPMENT, audio, plot=False)\n",
    "    fund_freq = fundamental_freq(AUDIOS_DEVELOPMENT, audio, plot=False)  \n",
    "    \n",
    "    features = pd.concat([fund_freq], axis=1)\n",
    "\n",
    "    features.index.name = 'audio_name'\n",
    "\n",
    "    all_features = pd.concat([all_features, features])\n",
    "\n",
    "display(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_all_features_old(audios, plot=False):\n",
    "    # extract features\n",
    "\n",
    "    time = time_domain(audios, plot)\n",
    "    freq = freq_domain(audios, plot)\n",
    "    spec = spectrogram(audios, plot)\n",
    "    mel = mel_spectrogram(audios, plot)\n",
    "    mfccs = mfcc(audios, plot)\n",
    "    hp = hpss(audios, plot)\n",
    "    zcrs = zcr(audios, plot)\n",
    "    poly_feats = poly(audios, plot)\n",
    "\n",
    "    # make unique dataframe\n",
    "    features = pd.concat([time, freq, spec, mel, mfccs, hp, zcrs, poly_feats], axis=1)\n",
    "    features.index.name = 'audio'\n",
    "\n",
    "    return features   \n",
    "\n",
    "def extract_all_features_for_single_audio(dir, audio, plot=False):\n",
    "    # extract features\n",
    "\n",
    "    time = time_domain(dir, audio, plot)\n",
    "    freq = freq_domain(dir, audio, plot)\n",
    "    spec = spectrogram(dir, audio, plot)\n",
    "    mel = mel_spectrogram(dir, audio, plot)\n",
    "    mfccs = mfcc(dir, audio, plot)\n",
    "    hp = hpss(dir, audio, plot)\n",
    "    zcrs = zcr(dir, audio, plot)\n",
    "    poly_feats = poly(dir, audio, plot)\n",
    "    fund_freq = fundamental_freq(dir, audio, plot)\n",
    "\n",
    "    # make unique dataframe\n",
    "    features = pd.concat([time, freq, spec, mel, mfccs, hp, zcrs, poly_feats, fund_freq], axis=1)\n",
    "\n",
    "    features.index.name = 'audio_name'\n",
    "\n",
    "    return features   \n",
    "\n",
    "def extract_all_features(dir, audios, function, plot=False, n_jobs=-1):\n",
    "    # Parallelize the feature extraction for each audio\n",
    "    all_features = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(function)(dir, audio, plot) for audio in audios\n",
    "    )\n",
    "    \n",
    "    # Combine all audio features into a single dataframe\n",
    "    features = pd.concat(all_features, keys=range(len(audios)), names=[\"audio_id\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_fundamental_freq = Parallel(n_jobs=-1)(\n",
    "#     delayed(fundamental_freq)(AUDIOS_EVALUATION, audio, plot=False) for audio in audio_eval\n",
    "# )\n",
    "\n",
    "# all_fundamental_freq_df = pd.concat(all_fundamental_freq, keys=range(len(audio_eval)), names=[\"audio_id\"])\n",
    "# all_fundamental_freq_df.index.name = 'audio_name'\n",
    "\n",
    "# all_fundamental_freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(df, dir):\n",
    "    for file, columns in columns_correlator.items():\n",
    "        with open(dir+file, \"w\", newline='') as f:\n",
    "            res = df[columns]\n",
    "            res.to_csv(f)\n",
    "\n",
    "def load_features(dir):\n",
    "    res = pd.DataFrame()\n",
    "\n",
    "    for file, columns in columns_correlator.items():\n",
    "        with open(dir+file, \"r\") as f:\n",
    "            res[columns] = pd.read_csv(f, header=0, index_col=0)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_ALL:\n",
    "    features_extracted_dev = extract_all_features(AUDIOS_DEVELOPMENT, audio_dev, extract_all_features_for_single_audio, n_jobs=-1)\n",
    "    save_features(features_extracted_dev, 'Features/dev/')\n",
    "else:\n",
    "    # features_extracted_dev = pd.read_csv('dev_features_extracted.csv', header=0, index_col=0)\n",
    "    # save_features(features_extracted_dev, 'Features/dev/')\n",
    "    features_extracted_dev = load_features('Features/dev/')\n",
    "\n",
    "features_extracted_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_ALL:\n",
    "    features_extracted_eval = extract_all_features(AUDIOS_EVALUATION, audio_eval, extract_all_features_for_single_audio, n_jobs=-1)\n",
    "    save_features(features_extracted_eval, 'Features/eval/')\n",
    "else:\n",
    "    # features_extracted_eval = pd.read_csv('eval_features_extracted.csv', header=0, index_col=0)\n",
    "    # save_features(features_extracted_eval, 'Features/eval/')\n",
    "    features_extracted_eval = load_features('Features/eval/')\n",
    "\n",
    "features_extracted_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_freq_eval = pd.read_csv('fundamental_freq.csv', index_col=0)\n",
    "# fund_freq_eval.head()\n",
    "\n",
    "# display(features_extracted_dev.head())\n",
    "# display(features_extracted_eval.head())\n",
    "\n",
    "# tot = pd.concat([features_extracted_eval, fund_freq_eval.drop(columns=['audio_name'], axis=1)], axis=1)\n",
    "# display(tot.head())\n",
    "# tot.to_csv('eval_features_extracted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_extracted_dev = features_extracted_dev.drop(columns=['audio'], axis=1)\n",
    "\n",
    "step4_dev_df = pd.concat([no_age_dev_df, features_extracted_dev], axis=1)\n",
    "step4_eval_df = pd.concat([step3_log_norm_eval_df, features_extracted_eval], axis=1)\n",
    "\n",
    "# step4_dev_df['word_per_second'] = step4_dev_df['num_words'] / step4_dev_df['audio_length']\n",
    "# step4_eval_df['word_per_second'] = step4_eval_df['num_words'] / step4_eval_df['audio_length']\n",
    "\n",
    "# step4_dev_df = step4_dev_df.drop(columns=['num_words', 'audio_length'], axis=1)\n",
    "# step4_eval_df = step4_eval_df.drop(columns=['num_words', 'audio_length'], axis=1)\n",
    "\n",
    "variances = [col for col in step4_dev_df.columns if 'var' in col.split('_')]\n",
    "\n",
    "step4_dev_df = step4_dev_df.drop(columns=variances, axis=1)\n",
    "step4_eval_df = step4_eval_df.drop(columns=variances, axis=1)\n",
    "\n",
    "step4_dev_df['char_per_second'] = step4_dev_df['num_characters'] / step4_dev_df['audio_length']\n",
    "step4_eval_df['char_per_second'] = step4_eval_df['num_characters'] / step4_eval_df['audio_length']\n",
    "\n",
    "step4_dev_df['words_per_second'] = step4_dev_df['num_words'] / step4_dev_df['audio_length']\n",
    "step4_eval_df['words_per_second'] = step4_eval_df['num_words'] / step4_eval_df['audio_length']\n",
    "\n",
    "step4_dev_df = step4_dev_df.drop(columns=['num_characters', 'num_words', 'audio_length'], axis=1)\n",
    "step4_eval_df = step4_eval_df.drop(columns=['num_characters', 'num_words', 'audio_length'], axis=1)\n",
    "\n",
    "display(step4_dev_df.head())\n",
    "display(step4_eval_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need standardize data\n",
    "scaler_join = StandardScaler().fit(step4_dev_df)\n",
    "\n",
    "step4_dev_norm_df = pd.DataFrame(scaler_join.transform(step4_dev_df), columns=step4_dev_df.columns)\n",
    "step4_eval_norm_df = pd.DataFrame(scaler_join.transform(step4_eval_df), columns=step4_eval_df.columns)\n",
    "\n",
    "display(step4_dev_norm_df.head())\n",
    "# display(step4_eval_norm_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_df = step4_dev_norm_df.copy()\n",
    "# X_train_val_df = X_train_val_df\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=300, n_jobs=5, random_state=341967)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val_df, ages_df, test_size=0.2, shuffle=True, random_state=341967)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_val)\n",
    "\n",
    "display(root_mean_squared_error(y_val, y_pred))\n",
    "\n",
    "_ = plot_error_distripution(y_pred, y_val, 0)\n",
    "#10.224589931506381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_df = step4_dev_norm_df.copy()\n",
    "# X_train_val_df = X_train_val_df.drop(columns=['age'], axis=1)\n",
    "\n",
    "# forest = RandomForestRegressor(n_estimators=800, n_jobs=5, random_state=341967)\n",
    "hist = HistGradientBoostingRegressor(random_state=341967, categorical_features=X_train_val_df.dtypes == 'object')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val_df, ages_df, test_size=0.2, shuffle=True, random_state=341967)\n",
    "\n",
    "hist.fit(X_train, y_train)\n",
    "y_pred = hist.predict(X_val)\n",
    "\n",
    "display(root_mean_squared_error(y_val, y_pred))\n",
    "\n",
    "_ = plot_error_distripution(y_pred, y_val, 0)\n",
    "#10.224589931506381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = sorted(zip(forest.feature_names_in_, forest.feature_importances_), key=lambda x:x[1] , reverse=True)\n",
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([imp for f, imp in imps[:81]]))\n",
    "\n",
    "mean_imps = np.mean([imp for f, imp in imps])\n",
    "\n",
    "print(len([f for f, imp in imps if imp > mean_imps]))\n",
    "\n",
    "drop_features = [f for f,imp in imps[81:]]\n",
    "# drop_features.append('audio_length')\n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "step4_dev_pruned_df = step4_dev_norm_df.drop(columns=drop_features, axis=1)\n",
    "step4_eval_pruned_df = step4_eval_norm_df.drop(columns=drop_features, axis=1)\n",
    "\n",
    "# step4_dev_pruned_df = step4_dev_norm_df.copy()\n",
    "# step4_eval_pruned_df = step4_eval_norm_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us try remove them\n",
    "X_train_val_df = step4_dev_pruned_df.copy()\n",
    "X_train_val_df = X_train_val_df\n",
    "\n",
    "forest_pruned = RandomForestRegressor(n_estimators=300, n_jobs=5, random_state=341967)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val_df, ages_df, test_size=0.2, shuffle=True, random_state=341967)\n",
    "\n",
    "forest_pruned.fit(X_train, y_train)\n",
    "y_pred = forest_pruned.predict(X_val)\n",
    "\n",
    "display(root_mean_squared_error(y_val, y_pred))\n",
    "\n",
    "_ = plot_error_distripution(y_pred, y_val, 0)\n",
    "#10.186472864996084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(forest_pruned.feature_names_in_, forest_pruned.feature_importances_), key=lambda x:x[1] , reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval_pred = forest_pruned.predict(step4_eval_pruned_df)\n",
    "\n",
    "with open(\"results.csv\", \"w\") as fout:\n",
    "    fout.write(\"Id,Predicted\\n\")\n",
    " \n",
    "    for id, y in enumerate(y_eval_pred):\n",
    "        fout.write(f\"{id},{y}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
